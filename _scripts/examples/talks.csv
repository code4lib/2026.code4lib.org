title,name1,name2,name3,description,length,time,ampm,day,dayno,group,spot,,
"Everything old is new again : What we could have learned from a book on COBOL from 1976, but didn’t",Chad Nelson,,,"Tech culture so often focuses on the future, it is easy to forget technology has a past. Yet, the long term history of software development, and the short term history of specific technologies, can provide a context to help us better understand our present situations, and inform our future choices. 
Using an 1976 book on writing better COBOL as a guide, this talk explores the ways ""new"" ideas are old, how ""bad"" ideas were good (in their context), and how we might use those insights to better navigate a tech zeitgeist that insists on throwing out the old again and again and again.",20,2:05,pm,2024-05-13,1,1,1,55,4
Everything could have been different: why computers are like that,Andromeda Yelton,,,"There are certain things we take for granted: bytes have eight bits, the internet uses packets. But these are decisions made by humans when other alternatives were on the table. In this talk, I’ll tell stories of some papers that were pivot points in the history of computing. How is the internet a response to nuclear war? How did a neuropsychologist and a tragic genius dream up neural nets? There will be an absolute banger of a bibliography for you to continue learning with.",15,2:25,pm,2024-05-13,1,1,2,,
The Last Two Women Standing:  libtechwomen's CodeClub,Bobbi Fox,Melani Polutta,,"Late in the year 2014, a call went out on the libtechwomen mailing list proposing the formation of CodeClubs, with the idea that reading other people's code is an essential part of learning to be a good developer.

Two CodeClub groups of 4-5 librarian/library workers each had a weekly virtual meeting, where code, usually from GitHub, was read and analyzed.

 All these years later, two of us, a cataloger and a library software developer, are still meeting weekly over Zoom.  However, not only are we reading code, now we're writing it!

We will describe the work we are doing and discuss the benefits of the CodeClub approach.

",10,2:40,pm,2024-05-13,1,1,3,,
Teaching coding online to full-time library workers,Justin de la Cruz,,,"My organization developed and ran a free 9-week online class titled ""Fundamentals of Health Data Science"" aimed at an audience of medical librarians. The course covers an introduction to Python in the context of working with a health-related dataset. The course was mostly asynchronous, with a group of 70 learners working on the same topics week-by-week, but we also hired librarians with Python experience to meet with learners and help them with the class. We think this format addresses some of the traditional challenges of completely asynchronous learning (isolation, lack of feedback) and completely synchronous classes (scheduling). This talk would present some background information on the class, the instructional approach we took, the challenges and successes we had, and the outcomes/feedback from class learners and helpers. This class will be repeated in the future, and presenting on it could help us receive more feedback on how to improve it going forward.",10,2:50,pm,2024-05-13,1,1,4,,
Whispering at any volume: Scalable speech recognition for all,Owen C. King,"Ryan ""Harpo"" Harbert",,"The American Archive of Public Broadcasting (AAPB) preserves and hosts over 150k audiovisual resources, with over 100k records available publicly. A collaboration between the Library of Congress and GBH (WGBH Boston), the collection spans over 100 years of media history.

Accurate transcripts remain critical for content discovery and accessibility, especially with such an extensive catalog. In September 2022, OpenAI released Whisper, a speech recognition model that revolutionized our transcription capabilities.

This presentation compares Whisper models against existing transcription approaches and demonstrates major improvements in accuracy and speed, while significantly reducing computing power. We examine test results across models and compare them to the popular Kaldi speech recognition toolkit, noting significant improvements in operational cost, especially with GPUs. In addition, we highlight Whisper’s multi-language capabilities and discuss common errors and biases.

Throughout, we showcase real-world examples along with source code for operating at multiple scales: starting from a single laptop, horizontally scaling to a group of machines, and vertically scaling to a Kubernetes cluster with GPU nodes. Here, we feature demos from “Chowda”, our new open-source web application for processing large batches of media through Whisper and additional workflow pipelines.",20,3:15,pm,2024-05-13,1,2,1,100,8
Leveraging AI Tools for Automating Metadata Extraction,Kristian Allen,Zoe Tucker,Leigh Phan,"With an increase in the accessibility of Generative AI and Large Language Models (LLM) we now have the ability to apply these tools against a variety of materials, including images, text, and audio.

The UCLA Library is experimenting with applying AI/ML tools against digital materials that would typically require significant human intervention to extract relevant metadata. Examples include text extraction via OCR, interview transcription, and metadata record generation from local digital library objects.

In this discussion we outline our experience applying some of the more common AI tools against a collection of digital library material with complex layouts, with the aim of building a foundation for the creation of a partially or totally automated metadata pipeline.

Topics and tools discussed:

* Optical Character Recognition (OCR) tools
* Named Entity Recognition and related trained models
* Applying LLM tools such as ChatGPT and Bard to extract metadata
",15,3:35,pm,2024-05-13,1,2,2,,
Multi-Modal Machine Learning to Enhance the Accessibility of Natural History Collections,Borui Zhang,Henna D. Bhramdat,Nicolas Gauthier ,"University libraries and museums hold vast collections of curated image data that document the natural and social world. While these data are, in theory, accessible to professionals and the general public, in practice, searching archival collections requires technical knowledge and relies on precise scientific terminology, which can be a barrier to its accessibility and broad appeal. Our project aims to enhance the accessibility of natural history collections data through artificial intelligence. We evaluate different “multi-modal” machine learning frameworks – combining insights from natural language processing and computer vision – and compare their performance to human subjects. We then develop a system that unifies collections images and both novice-and expert-level natural language descriptions. We apply our system to museum collections at the Florida Museum of Natural History to assess its effectiveness for querying existing databases. Preliminary findings suggest that larger models may not consistently outperform their smaller counterparts in certain image-to-text tasks. The workflow is transferable to broader text- and image-centric museum and library collections, laying the groundwork for custom natural-language search tools tailored to unique collections.   ",10,3:50,pm,2024-05-13,1,2,3,,
Enhancing Cataloging of Electronic Government Documents with Programming and OpenAI,Greta Heng,Kate Holvoet,Nerissa Lindsey,"The advent of AI products like OpenAI ChatGPT has surged into the learning and library scenes, prompting us to contemplate how to harness these advances to enhance our work and envision the future of our endeavors. Especially within the domain of electronic resources and born-digital materials, developers have utilized their programming skills to extract essential resource descriptors from source files. The fusion of these programming skills with AI technologies empowers metadata and cataloging professionals to redefine and automate their workflows to a certain extent, resulting in increased efficiency.

This presentation aims to demonstrate the experiments we did, utilizing Python and OpenAI APIs to enhance the efficiency of cataloging electronic government publications in the MARC format. We will not only present our ongoing research and discuss the challenges we’ve met but also engage in an insightful dialogue concerning the potential impact of these technologies on the evolution of cataloging practices in the future.",10,4:00,pm,2024-05-13,1,2,4,,
Building a Walking Tour App to Increase Knowledge and Empathy for Complex Historical Sites,David A. Wallace,Siddique Motala,,"Using a wide range of third party software tools and services we have built a customizable walking tour Android app that integrates Geographic Information Systems and archival content. The first iteration of the app has been demonstrated on the site of District Six in Cape Town, where some 60,000 people of color were forcibly removed and their homes, business, and social institutions were demolished by the apartheid government. The app guides users through the historically erased landscape and allows them to understand what used to exist at specific geolocations through a series of descriptive text and historical visual imagery. Users can even use a “slider” to line up and compare the contemporary landscape with the historical built environment before it was demolished. We feel that the affective impact of the erased landscape and what was lost is rendered more meaningful by guiding users to the exact location where homes, houses of worship, business, and social institutions once stood. The app is currently being piloted by the District Six Museum as a tool to train a new generation of walking tour guides. More broadly, the app is designed to be site independent, meaning any physical landscape / location can be used as the basis for a walking tour and where creators can incorporate their own unique sites, points of interest, maps, content, and stories.",10,4:10,pm,2024-05-13,1,2,5,,
[Re]Thinking Digital Infrastructure: Centering Humans in Integrated Systems Work,Mark Lane,Rebecca French,Bodeene Amyot Cairdeas,"In this talk, librarians and archivists from an academic library will discuss establishing and cultivating a Digital Infrastructure Working Group (DIWG). The DIWG was born out of a need to create interoperability between metadata, preservation, discovery, and access systems for archival materials, and demanded a radical rethinking of prior organizational approaches to digital asset curation, including leveraging available APIs and vendor contracted system development. Learn how team members approached technological work by centering each other’s well-being and intentionally making visible the often-invisible human labor invested in developing and maintaining digital infrastructure. We will share how the DIWG challenged organizational silos, established cross-departmental communication and workflows, and cultivated a climate of professional support and human-centered care in a pandemic and post-pandemic workplace. Key takeaways include recommendations made to library administration for digital infrastructure curation moving forward.",15,4:20,pm,2024-05-13,1,2,6,,
Institution repository collection development with web scraping,Brian Clark,,,"Many institutions report low rates of self-archiving with their institutional repositories, requiring repository managers to actively seek content to expand collections. This presentation discusses a method for collecting articles and metadata from open repositories using Beautiful Soup and Selenium as web scraping tools. Thousands of articles and corresponding descriptive metadata were quickly and easily added to an IR using this method, increasing visibility of research and engagement with the IR.",10,4:35,pm,2024-05-13,1,2,7,,
Rethinking Digital Asset Management at University of Texas with OCFL in Fedora 6,Addison Ashworth,Nicole Currens,Larry Yang,"We at University of Texas at Austin Libraries are leveraging the standards-based format, OCFL (via Fedora), to create modular and easily maintained digital repositories. To address pain points maintaining, enhancing, and especially migrating our services over the years, we are rethinking how we build them from the ground up. We are using standards-based formats like OCFL to create modular, API-driven repositories that can be upgraded or replaced without compromising the data preserved within. Additionally, over the past few years, we have transitioned to Kubernetes and this has had a large impact on how we look at our app architecture as a whole. We’d like to share our experiences and outcomes reconstructing a repository with this mindset last year, and how we are applying the lessons learned to our current and future projects.",10,4:45,pm,2024-05-13,1,2,8,,
Encoding Reparative Description: Developing Tools to Analyze Problematic Finding Aids,Jesse Johnston,Ella Li,,"Over the past few years, more archives and archivists have been working on enhanced description projects that can address past inequities, erasure, or incorrect representations in description. This work has been variously described as reparative description, mindful description, conscious editing, and by other names. Whatever it is called, while this work is typically grounded in slow, relationship-based work within and outside of archives, the identification of problems and addressing of changes can benefit from computational approaches. Stemming from the “ReConnect/ReCollect” project at University of Michigan, which has surveyed the extent and legacy of colonial collections extracted from the Philippines since the late nineteenth century, we report on work to analyze more than two hundred finding aids with the development of Python-based analysis tools. We are not advocating for technical solutions to fundamental problems, but we are interested in showing how such automation can help to expand the project of reparative description. Our presentation will report on work by students and faculty at the University of Michigan School of Information, collections managers and curators across the University, and systems managers at these various institutions worked together to aggregate finding aid metadata, analyze that descriptive information for potentially harmful, outdated, or problematic terminology and tone. We conclude with information about the code that we used, and the potential challenges and benefits of adopting automated approaches in the implementation of reparative description.   ",20,9:15,am,2024-05-14,2,3,1,50,4
Cleaning metadata in bulk through batch editing,Corey Halpin,,,"Collections of any significant size or age often include data that is not consistent across records. Frequently this data can be normalized by applying a large number of small but similar changes across groups of records. Tools that support bulk changes to groups of records can make this kind of collection maintenance practical without a large investment of time in manually re-cataloging individual records. The Metavus digital collections platform includes a BatchEdit plugin explicitly designed to support this sort of workflow. Records to modify are selected by adding them to a folder, either individually or by adding all the results from a search. For controlled vocabularies, BatchEdit can clear a field entirely, clear specific terms when present, or add specific terms. For fields that contain textual data (e.g., Paragraph, URL, Email, etc), BatchEdit can prepend data, append data, and perform find/replace operations. These operations can be applied in combination to quickly normalize records to follow a consistent set of standards.",10,9:35,am,2024-05-14,2,3,2,,
kwalk: a simple program to crosswalk metadata for repository uploads,Kirsten Vallee,,,"Kwalk is a program that lets us write a simple crosswalk that we can apply to each batch of metadata as we receive it and have multiple crosswalks for multiple projects as we work on them in an intermixed fashion. The program allows us to apply special functions to modify date formats, combine literal and field name text, generate uniform upload URLs, and much more.  

University of Chicago’s Center for Digital Scholarship has been utilizing this program to better edit metadata for batch upload to Knowledge@UChicago. There are plans to share this software in the future as it is platform agnostic and has a potential wide range of use cases.  ",10,9:45,am,2024-05-14,2,3,3,,
Are We Donne Yet? The Decade Long Project of Converting an Annotated Bibliography from Print to Database,Matthew Sherman,Brent Nelson,,"“Do you think we could create a searchable database?"" Do questions like this fill you with dread? Or maybe excitement? They can and should do both! This superficially simple question led an inter-institutional team of librarians and scholars on a decade-long project of various stages to convert the four-volume John Donne: An Annotated Bibliography of Modern Criticism into a user-expandable, searchable database. The project started as a two-person project that was later integrated into a larger grant funded team. We will share our experience as an interdisciplinary team implementing a text-base digital humanities project that has become an open-access, fully searchable Drupal database that will soon be available to the public.  We will outline some of the challenges we faced taking this from print into electronic and the ways in which the digital format forced a kind of precision and standardization that is not common in humanities resources, and conversely, the challenges of designing a database that could accommodate the ambiguities and vagaries of the materials humanities scholars work with. We'll also highlight considerations from throughout the project, from early, fundamental questions regarding database and workflow design, to more recent pragmatic problems such as standardizing author, subject, and work indexes from a body of work that spans 4 decades of evolving scholarship.",10,9:55,am,2024-05-14,2,3,4,,
OCR and PDFs - the possibilities and limitations for accessibility,Sarang Joshi,,,"In order to serve its patrons, NYPL is constantly finding new ways to make more content easily accessible through digitization. In order to expand on our research catalog and get the most out of digitization, we have started work on using Optical Character Recognition (OCR) on scanned books to not only provide digital copies, but to improve accessibility with tagged and searchable PDFs of our research materials. In this talk, we’ll explore the technical details of our new PDF creation pipeline, as well as both the possibilities and limitations we’ve encountered in using OCR data.",10,11:05,am,2024-05-14,2,4,1,55,4
"Creating Accessible Documents As Non-Authors: Tools, Strategies, & The False Idol of AI",Christina Cutler,,,"Documents are only accessible if all users can examine their contents, yet many of the ""openly accessible” articles, books, and documents at libraries are often not compatible with text-to-speech readers, preventing low-vision users from accessing their information. The barrier to uploading accessible material is high, as making these documents accessible for low-vision users can be an arduous task for those who are not authors of the material, and there is little recognition when it is performed by either the author or library staff. As a result, accessibility work is often done haphazardly, leaving large swathes of online library content inaccessible for low-vision users. Grappling with this as I worked on projects with my University’s Institutional Repository, I performed a research project to examine our accessibility workflows and update them to be more rigorous and time efficient. In a talk targeted at fellow time-pressed accessibility advocates, I will discuss what the research tells us makes documents accessible, how to triage accessibility requirements, and the available tools and strategies for making documents accessible efficiently, including several emerging AI based tools which show tremendous promise.",20,11:15,am,2024-05-14,2,4,2,,
"Change is Afoot: What WCAG2.2, WCAG3, ..., WCAG9000? Mean for Libraries","Katherine ""Kate"" Deibel",,,"As of October 2023, the Web Content Accessibility Guidelines (WCAG) were officially updated to version 2.2. This new W3C recommendation introduces nine new success criteria (ranging from A to AAA) and a handful edits to existing criteria. This talk will discuss the potential implications of WCAG 2.2 for our library web content and provide potential fixes if necessary. Moreover, I will discuss how new WCAG recommendations are crafted, including some preliminary discussion and cautions regarding WCAG3 and the future of the web accessibility. ",15,11:35,am,2024-05-14,2,4,3,,
Beyond Band-Aids: Rethinking Accessibility Widgets,Robin Davis,Meredith Wynn,,"Third-party accessibility widgets are often advertised as a quick fix to make your website accessible. The truth is that no third-party widget can replace a fundamental commitment to designing and maintaining accessible web content… But is there another use case for this kind of tool? We recently launched our twist on the “band-aid” accessibility widget: a set of optional tools to increase perceptibility of our main library website, with a special focus on features for neurodiverse users, such as a dyslexic-friendly font. A conversation with a disability advocacy student group inspired us to build this open-source widget that cedes some control over our site design to the user to improve accessibility, readability, and their overall experience. These features are a step toward ensuring that our diverse population of users has equitable access to our website content. In this talk, we’ll discuss how we used universal design principles to design this widget, give a demonstration, and share user feedback.",10,11:50,am,2024-05-14,2,4,4,,
Wrangling the Past: Using Python to Prepare Legacy Digital Collections and ETDs for Preservation,Grayson Murphy,,,"This presentation will explore the ways in which Python scripts, both large and small, are effective tools for normalizing large amounts of data that make up disparate digital collections and preparing them for digital preservation. After setting up a digital preservation program, our institution was faced with a backlog of 33 digital collections, totaling 3.7 TB of data that was generated over the course of 15 years, as well as 17 years’ worth of electronic theses and dissertations (ETDs). We set out to appraise, organize, and package each collection for ingest into our digital preservation repositories with the aid of python scripts. The presenter will detail how digital preservation principles dictated scripting decisions, as well as considerations for implementing similar strategies at other institutions, including building workflows and troubleshooting scripts. The presenter will end with lessons learned from the completion of this large-scale project. ",15,1:30,pm,2024-05-14,2,5,1,45,3
We Can Build That: Custom Python Applets for Digital Preservation Processing,Magnus Berg,,,"When processing digitized and born digital collections many archivists and librarians are faced with two potential pathways 1) using open sources tools, many of which require intermediate to advanced technical skills or 2) using specialized software, most of which are not tailor made to digital preservation use cases. The Digital Scholarship Unit (DSU) at the University of Toronto, Mississauga Library faced this same dilemma when digitally preserving the EP Media Fonds, which is comprised of both analog videotape and born digital video. While digital video converters abound, neither out-of-the-box commercial nor open source solutions quite fit the bill. Software like Adobe Media Encoder was not capable of converting formats such as XDCam and alternatives like FFmpeg were too complex for some staff to utilize effectively. For this reason, the DSU sought to develop custom applets using Python that streamline the preservation process by providing a simple graphical user interface with a custom back-end that allowed novice users to utilize libraries like FFmpeg and Bagit to convert, create preservation and access copies, and package digital media for preservation purposes without requiring any knowledge of command line interfaces or video transcoding. This presentation will review the development process and how similar tools can be developed for use by student and non-technical staff in libraries and archives. ",15,1:45,pm,2024-05-14,2,5,2,,
Integrating Digital Library Platforms: The DIY Approach,Phil Salvador,Travis Brown,,"What's more frustrating than setting up a library system? Setting up TWO library systems! Especially when they won't talk to each other.

When the Video Game History Foundation started building their library, they had trouble getting their archives management platform (ArchivesSpace) to play nicely with their digital repository (Preservica). While Preservica has built-in integration features for ArchivesSpace, the VGHF team wanted more flexibility and functionality than what the official tools could offer. Using the APIs for both platforms, they developed their own CLI program that gives them more control over how they synchronize metadata.

In this talk, you'll hear from the odd couple of VGHF's library team — an academic librarian and a platform engineer — about how they combined their skillsets to build a tool that worked better than the off-the-shelf solution. Will the engineer learn how to do a MODS crosswalk? Will the librarian learn how to debug Python?",15,2:00,pm,2024-05-14,2,5,3,,
"Secrets and Lies: Exploring and Exploiting APIs That Are Undocumented, Underdocumented, or Misdocumented",Susan Hoover,,,"Sometimes you find a service or website that serves up information that you want to use, and you want to automate your calls to it, but there's no documentation, or whatever documentation exists is less than helpful or even wrong. This presentation will discuss and demonstrate some techniques and tools for experimenting with calling APIs to get the results you want. Tools mentioned may include curl, wget, jq, browser developer tools, Postman. Demonstration may be prerecorded for reliability and efficiency.",20,4:00,pm,2024-05-14,2,6,1,60,4
"Think Big, Act Localhost: using large scale data techniques at smaller scales for ad hoc data analysis",Steve Meyer,,,"Even the largest libraries generally do not work with data that would qualify as Big Data, such as the log data represented by Google searches or Facebook posts. That puts some libraries in a bit of a bind, what we might call a ""Medium Data"" problem. The catalog data of a larger library, measured at the scale of a many millions of records, is interesting enough to be the subject of intra-data set analysis, but large enough that it can exceed the simple programming and scripting techniques used for ad hoc analysis.

This talk will present intermediate level programming techniques for working in this intermediary space between larger infrastructure projects and ad hoc, experimental computer programming. It will discuss strategies for how simple scripts that do not utilize databases or search indices can still process millions of records efficiently. The canonical example used to illustrate these techniques will be a simple FRBRization clustering algorithm for approximately 10 million MARC records. FRBR, or Functional Requirements for Bibliographic Records, specifies a model in which multiple instances of the same work, such as the print, electronic and microform copies of the same title, can be clustered together as a single unit.",20,4:20,pm,2024-05-14,2,6,2,,
Automation of Subject Analysis in Collection Evaluations,Sephra Byrne,Karen Harker,,"Every year we analyze an assortment of subject-based collections in order to make recommendations for program accreditation, enhancement purchases, and promotion. We typically start this process with a needs assessment and with analyzing collection-level trends. The final step of the evaluation process is to do a deep dive into each subject to examine a selection of raw and calculated measures (such as ILL requests, usage, recency, and the percent of Choice’s Outstanding Academic Titles list held) and assess whether the subject is strong, meets needs, or needs improvement. This process used to be completed manually, but with an average of 116 subjects per collection, this can be a very time-consuming, complex, and subjective process. In this presentation, we will discuss how we automated this process using a combination of SQL, Python, and regression to condense data about holdings, usage, ILL, and more into three scores for sufficiency, quality, and interest. We will also discuss how we use these scores to make recommendations that streamline the purchase process and enable targeted usage of our limited funding.",10,4:40,pm,2024-05-14,2,6,3,,
Move Fast and Fill Things: Using Python for Full-Text Retrieval,Tiffany Tawzer,Christopher Beger,,"In 2022, a growing systematic review service at a 5 -person academic health sciences library prompted staff to seek creative ways to manage workload while still providing comprehensive service. Access Services staff, who were novice but enthusiastic coders, developed a Python script to expedite retrieval of licensed and Open Access full-text articles (sometimes numbering in the hundreds) for the review teams.  After incorporating more complex Python libraries and API connections to dramatically reduce retrieval time, staff were inspired to adapt the script for interlibrary loan services. This streamlined lending processes in a way that increased the volume of filled requests while decreasing the overall time commitment. With such a small, versatile team, staff time is paramount, and this opened up opportunities for additional projects and responsibilities. Staff continue to refine the script and adapt it in new ways.",10,4:50,pm,2024-05-14,2,6,4,,
Lessons learned: How to get traction with AI and start building,Raiden van Bronkhorst,Charlie Collett,,"Getting a grip on AI takes time, but after iterating through our first AI project, a method of comparing MARC records, we’ve gained enough skills and perspective to begin leveraging AI as a potential tool in our technology stack. There are many paths for incorporating AI, building a model from scratch, fine-tuning an existing model, or using a large language model through an API. However, we’ve learned through success and failure that there is a lot to consider before jumping into a solution. In this talk we’ll discuss how we’ve gotten up to speed, changed the way we collaborate, and the questions we now ask ourselves first when we consider using AI for a problem. We’ll talk about our current process for working with AI, and wrap up by sharing our ongoing AI work.",15,11:15,am,2024-05-15,3,7,1,50,4
Exploring the Flip Side of Explainable AI:  Unexplained Expert Systems and Cultural Acceptance for the Unexplained,Wilhelmina Randtke,,,"Current buzz about artificial intelligence tends to present the excitement as coming from technological advances like speed to allow a real time conversation and better quality responses from chatbots and generative artificial intelligence.  This talk explores how cultural expectations, rather than technology, may be what recently reached a tipping point to put artificial intelligence into the public spotlight.  From a non-technological perspective, explainability is a major difference between machine learning and traditional software which encoded and applied logical rules.  Popular acceptance and embrace of machine learning requires a comfort level with not having an explanation for why the software does what it does.  This talk explores how past developments in how we interact with traditional software accustomed the general public to not getting an explanation, even for very explainable software.  Technology tools like skip logic in forms prevent us seeing the big picture within a logical system.  Cloud computing primed us to expect updates over time and accept constant changes outside our control to tools that we use daily.  When even imminently explainable software is not explained, explainability no longer matters.  This opens the way for the public to embrace machine learning.",10,11:30,am,2024-05-15,3,7,2,,
Aligning Keywords from Long Form Prose to Controlled Vocabulary,Kio Polson,Jane Greenberg,Scott McClellan,"HIVE-4-MAT is a linked-data, automatic indexing application for vocabularies related to material science. In the past few months, work has been done to improve the performance of the keyword alignment algorithm so that it is faster, more accurate, and more flexible at the expense of precision. This presentation reports on the lessons learned in the process of refactoring this keyword alignment algorithm. Since HIVE-4-MAT has a somewhat broad scope, it provides a good use case for analyzing a keyword alignment pipeline from raw article text scraping to keyword extraction to keyword matching and alignment. The presentation will touch topics such as common pitfalls of web scraping, different strategies for preparing raw text for keyword extraction, the differences in goals between keyword extraction and keyword alignment, and the potential benefits and drawbacks of utilizing the concept of string distance in keyword alignment algorithms.",15,11:40,am,2024-05-15,3,7,3,,
Automating Metadata Hygiene to Improve Economic Research Discoverability,Scott St. Louis,,,"Fed in Print is an application indexing papers, publications, and speeches from twelve Federal Reserve Banks and the Board of Governors. By presenting metadata to larger discovery services, including Research Papers in Economics (RePEc), Fed in Print serves as a discoverability driver for Federal Reserve System research. As a service indexing roughly fifty thousand resources to date, link rot is a stubborn problem for the application; content migrations cause links to break. This talk will report on a long-term project in progress, revealing tools and considerations that can be helpful in designing similar projects. Utilizing the recently deployed Fed in Print API, more than fifty thousand file URLs indexed in the application were tested for 404 (“resource not found”) errors and other discoverability problems. The broken URLs have been divided among collaborators throughout the Federal Reserve System. This talk will focus not only on the tools involved, but also project complications and benefits. Tools include Python, Postman, Excel, and Teams. Complications include coding challenges and individual capacity limitations. Benefits other than greater discoverability include API bug detection and, hopefully, initiation of proactive conversations between librarians and content professionals responsible for publishing material indexed in the application before future migrations.",10,11:55,am,2024-05-15,3,7,4,,
Common Pitfalls of Project Management In Academic Libraries,Erik Beck,,,"Few libraries have a project management office that can carry out their projects in a systematic matter.  Developers, supervisors and technicians are often called upon to conduct their own project management while also doing much of the project work themselves.  In this presentation I will cherry-pick the choicest bits of wisdom from the PMBOK Guide and tailor them for library scenarios where work units are divided into functional departments and project managers serve multiple roles.  I will structure this talk around common issues that arise with library projects and show where project management best practices can be applied to remedy those issues.  Attendees will gain knowledge for successfully planning projects and for controlling work over the course of a project.   For this presentation I will be drawing on the PMI 99-001 standards for project management as well as mixing in a few of my own insights.",15,3:35,pm,2024-05-15,3,8,1,65,5
Make Time for Design: Integrating Design Ops Creates Big Value for Small Teams,Bridget Burke,,,"In higher education IT, there are more developers than dedicated designers on a team. Oftentimes, there are no designers allocated to the ongoing work of a digital product. Designers and design work feels siloed and mysterious. In an environment that values continuous improvement and shipping work, how do we advocate and make time for design, UX, and accessibility without causing bottlenecks or frustration? This talk explores how investing in design ops and design systems can bring together design and engineering and creates a culture of confidence, collaboration and creativity with our small teams. 

The U-M Library Design System team formed in 2017 with two members. We created the design system to reduce duplication effort and amplify our impact on the UIs we were building. But using a design system alone is not enough. In 2020, we began focusing on the bigger picture, investing in various design operations for our tech division. This “invisible” work has created a collaborative, design-led mindset in our product teams as we work together building accessible, usable digital applications. Everyone can do design with the right tools and systems.",10,3:50,pm,2024-05-15,3,8,2,,
Community Source Software: Complex collaborations as told by two Program Managers,Arran Griffith,Emily Porter,,"The decision to implement open, community-supported technologies can bring with it complexities and efforts not always experienced by the use of proprietary solutions. “Open Source”, by definition, can also lead to confusion and misunderstanding of the true cost of implementation, as well as underestimation of the importance of collaboration and advocacy. 

Program Managers are in a unique position to help build collaborative bridges as a means of uniting users, supplying actionable use cases, and fostering open dialogue. In this talk, we will discuss commonalities and differences in our roles and how our respective organizations must collaborate, both internally and cross-organizationally, in an attempt to meet the needs of multiple technical and user communities in the digital repository realm.

As Program Managers of both an open-source, community-supported software, and an academic library’s digital repository, which uses said software, presenters will highlight and discuss their perspectives, methodologies and their on-going work to attempt to solve issues faced by many others within their own communities. Through exploratory work to identify barriers to adoption, migration and paying technical debt, we will discuss common goals and challenges to sustaining preservation repository technology across complex open frameworks.",15,4:00,pm,2024-05-15,3,8,3,,
"One Team, Multiple Initiatives: Project Management and Collaboration Across a Library Team",Stephanie Mannheim,,,"How does a team balance multiple projects and conflicting priorities? Using the New York Public Library (NYPL)’s eReading team as a case study, this presentation will walk attendees through methods of collaboration and resource sharing from a project management perspective. NYPL has developed multiple eReading apps that are used for different sets of patrons: SimplyE, where patrons from participating libraries can check out and read eBooks on their devices, and Open eBooks, where students at Title I schools can access free eBooks. On our eReading team, developers contribute to both projects, while specialized project and product managers lead each initiative. We will discuss best practices and lessons learned through our project management methods.",10,4:15,pm,2024-05-15,3,8,4,,
The Compassion of DevOps,Bess Sadler,,,"A strong DevOps culture helps teams deliver high quality products on predictable schedules. But in my experience, the strongest DevOps culture is the one that recognizes suffering and offers compassion to a technology workforce that is often over tasked and under resourced. This talk will discuss DevOps best practices, but also how the lack of these practices can take a toll on our bodies and spirits, in ways that are not often acknowledged. ",15,4:25,pm,2024-05-15,3,8,5,,